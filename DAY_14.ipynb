{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89ec3191"
      },
      "source": [
        "### Project: Code-Focused Inference\n",
        "\n",
        "Load a pre-trained GPT-2 model and configure it to answer *only* questions related to Python coding.\n",
        "\n",
        "1. **Load Model and Tokenizer:** Load a suitable pre-trained GPT-2 model and its corresponding tokenizer.Use `transformers.AutoModelForCausalLM` and `transformers.AutoTokenizer`. A smaller model like `gpt2` or `gpt2-medium` might be sufficient.\n",
        "2. **Implement a Filtering Mechanism:** Use prompt techniques\n",
        "3. **Generate Response:** If the prompt is deemed a Python coding question, generate a response using the loaded GPT-2 model.\n",
        "4. **Handle Non-Coding Questions:** If the prompt is not related to Python coding, return a predefined message indicating that the model can only answer coding questions.\n",
        "5. **Test:** Test the implementation with various prompts, including both Python coding questions and non-coding questions, to ensure the filtering mechanism works correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fc15ed8"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import re\n",
        "\n",
        "MODEL_NAME = \"openai-community/gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d3f00c6"
      },
      "source": [
        "def generate_python_response1(prompt: str, max_length: int = 100):\n",
        "    \"\"\"\n",
        "    Generates response only if prompt is related to Python coding.\n",
        "    \"\"\"\n",
        "\n",
        "    if not is_python_coding_question(prompt):\n",
        "        return \" This model can only answer Python coding-related questions.\"\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "\n",
        "Question: {prompt}\n",
        "\n",
        "Python Code:\n",
        "\"\"\"\n",
        "    inputs = tokenizer(system_prompt, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.7,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return response"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Write a program to create a tuple\"\n",
        "print(generate_python_response1(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEzvYmmOrP5t",
        "outputId": "cfacdfd4-61c7-4be5-a6ef-b42316624728"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Question: Write a program to create a tuple\n",
            "\n",
            "Python Code:\n",
            "\n",
            "import sys import random def tuple(x): x = x + 1 return tuple(y) def tuple2(x, y): return tuple2((x,y)) def tuple3(x:xs, y:ys): return {x, x + y}\n",
            "\n",
            "Answer: Python code\n",
            "\n",
            "I wrote a program that creates a tuple and then prints the result.\n",
            "\n",
            "The program does\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"What is the capital of France?\"\n",
        "print(generate_python_response1(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_at1kw5xr60x",
        "outputId": "92e5290d-285a-42fc-b1e9-0913156e07c7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This model can only answer Python coding-related questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0a65e17"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import re\n",
        "\n",
        "MODEL_NAME = \"gpt2-medium\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afbee999"
      },
      "source": [
        "def generate_python_response2(prompt: str, max_length: int = 150):\n",
        "    \"\"\"\n",
        "    Generates response only if prompt is related to Python coding.\n",
        "    \"\"\"\n",
        "\n",
        "    if not is_python_coding_question(prompt):\n",
        "        return \" This model can only answer Python coding-related questions.\"\n",
        "\n",
        "    # Prompt engineering to restrict domain and guide generation\n",
        "    system_prompt = f\"\"\"\n",
        "\n",
        "Question: {prompt}\n",
        "\n",
        "Code:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(system_prompt, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.7,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Write a program to create a list\"\n",
        "print(generate_python_response2(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ymc0KCwlwIb",
        "outputId": "9f464deb-34f0-444c-b1f8-580f65577b49"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Question: Write a program to create a list\n",
            "\n",
            "Code:\n",
            "\n",
            "import std.algorithm.comparison : equal;\n",
            "\n",
            "auto list = equal(list, \"foo\", \"bar\");\n",
            "\n",
            "assert(list == \"foo\");\n",
            ",\n",
            "\n",
            "The program above prints:\n",
            ", The program above outputs:\n",
            ".\n",
            "\n",
            "Note that the equal() function is used to compare two lists. It compares the elements of the two lists, and returns true if the first list is equal to the second list.\n",
            ", and the program above outputs:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1=\"What is photosynthesis?\"\n",
        "print(generate_python_response2(prompt1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln6LCNDbsyLH",
        "outputId": "fa676d1c-ddb1-4ee4-dfd6-989fd0896f23"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This model can only answer Python coding-related questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4SFiZ79-s3nh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}